# Speech Visualization

This will take an audio file and visualize various characteristics. Currently, you can see when speakers speak in the file and when there is voice activity (and conversely no voice activity). Additionally, you can play any given segment of speech by a given speaker by pressing the play button. If you press the loop button right next to it, the segment will play on repeat.

## Contents

- [Docker / Podman image](#docker-podman-image)
- [Manual installation](#manual-installation)
- [Usage](#usage)
- [VRS data extraction](#vrs-data-extraction)
- [Troubleshooting](#troubleshooting)

## Docker / Podman image

    git clone https://research-git.uiowa.edu/uiowa-audiology-reu-2022/speechviz.git
    cd speechviz
    docker build . -t speechviz

Note that the above commands build the image with PyTorch CPU support only.
If you'd like to include support for CUDA, follow the instructions for using the
[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html)
and add `--build-arg cuda=true` to the `docker build` command above:

    docker build --build-arg cuda=true . -t speechviz

## Manual installation

    git clone https://research-git.uiowa.edu/uiowa-audiology-reu-2022/speechviz.git
    cd speechviz

### Setup the interface

    npm install
    npm run mkdir
    python src/scripts/db_init.py

### Install script dependencies

If you'll be using `extract-vrs-data.sh`, you will need to install [VRS](https://github.com/facebookresearch/vrs).
To use `process_audio.py`, you will need to install [audiowaveform](https://github.com/bbc/audiowaveform)
and [ffmpeg](https://ffmpeg.org/). The remaining dependencies for `process_audio.py` can be installed using `pip` or `conda`.

#### pip

To install with PyTorch CPU support only:

    pip install --extra-index-url "https://download.pytorch.org/whl/cpu" -r requirements.txt


To install with PyTorch CUDA support (Linux and Windows only):

    pip install --extra-index-url "https://download.pytorch.org/whl/cu116" -r requirements.txt

#### conda

    conda env create -f environment.yml

## Usage

Before running the server, you'll need to process some audio. To do so, run:

    python3 src/scripts/process_audio.py FILE

For more information, run:

    python3 src/scripts/process_audio.py -h

<br><br>

To start the server, run:

    npm start

and then open http://localhost:3000 in your browser.

By default, the server listens on port 3000. To specify a different port, run with the port option:

    npm start -- --port=PORT

where PORT is the port you want the server to listen on.

<br><br>

To actually display any audio, you need to:
1. Add the audio file to the `data/audio` directory.
2. Add the json file for the waveform (generated by the pipeline) to the `data/waveforms` directory.
3. Add the json file for the segments (generated by the pipeline) to the `data/segments` directory.

For example, for the file `example.mp3`, there should be `data/audio/example.mp3`, `data/waveforms/example-waveform.json`, and `data/segments/example-segments.json`.

## VRS data extraction

    git clone https://github.com/facebookresearch/Aria_data_tools.git
    podman build . --network=host -t aria_data_tools
    podman run -it --volume <your_local_data>:/data aria_data_tools:latest

It is reccomended you use bigcore for this as we have encountered VRS build issues on Windows 11/Mac os.

To get video from a VRS file, while in the container cd to the folder the vrs file is in. Then run:

    vrs extract-images file.vrs --to image-folder + 1201-1

where + 1201-1 can be substituded for whichever sensor you want to get video from. 1201-1 is the first SLAM left camera. A complete list of sensors used with ARIA can be seen [here](https://facebookresearch.github.io/Aria_data_tools/docs/sensors-measurements/#naming-conventions-for-all-tools)

Next cd to where ./create-video.sh is, in the main Speechviz folder then run

    src/scripts/create-video.sh path/to/image-folder

This will give you an mp4 of all the images extracted without any of the audio.

So the next step is audio extraction. For audio run:

    vrs extract-audio file.vrs --to image-folder

Next write exit to leave the container. The next step is to use ffmpeg to rotate the video 90 degrees clockwise because by default the images are recorded rotated 90 degrees counterclockwise.

    ffmpeg -i vid.mp4 -vf transpose=1 output.mp4

After that you will have to change the audio file you got from wav to mp3 so it can be combined with the mp4.

    ffmpeg -i input-file.wav -vn -ar 44100 -ac 2 -b:a 192k output-file.mp3

Lastly combine the mp4 and mp3 to get a video with audio.

    ffmpeg -i input.mp4 -i input.mp3 -c copy -map 0:v:0 -map 1:a:0 output.mp4

At this point if you want to then analyze the video with speechviz move the video to data/video and run

    python3 src/scripts/process_audio.py data/video

## Troubleshooting

If installing on Bigcore, you are likely to run into the following error:

    ERROR: Could not install packages due to an OSError: Proxy URL had no scheme, should start with http:// or https://

To resolve this, run the following command:

    http_proxy="http://$(echo $http_proxy)" && https_proxy="http://$(echo $https_proxy)"

<br><br>

If you receive this error:

    subprocess.CalledProcessError: Command '['ffmpeg', '-y', '-i', 'input_file_here' 'output_file_here']' returned non-zero exit status 127.

Running

    conda update ffmpeg

should resolve the issue.
