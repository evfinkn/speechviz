# Speech Visualization

This will take an audio file and visualize various characteristics. Currently, you can see when speakers speak in the file and when there is voice activity (and conversely no voice activity). Additionally, you can play any given segment of speech by a given speaker by pressing the play button. If you press the loop button right next to it, the segment will play on repeat.

## Installation

```
git clone https://research-git.uiowa.edu/uiowa-audiology-reu-2022/speechviz.git
cd speechviz
npm install
npm run mkdir
python db_init.py
```
To use process_audio and the pipeline, you'll need to first make a conda environment. If installing on Linux or Windows, you can create the environment using the environment.yml file (you will still need to install audiowaveform and ffmpeg manually (check `conda list` for ffmpeg, might've been installed automatically)):
```
conda env create -f environment.yml
```  
<br><br>
Otherwise, you can create the environment manually:
```
conda create -n speechviz
conda activate speechviz
conda install pytorch numpy torchvision torchaudio cudatoolkit=11.3 -c pytorch
```
Then, install [pyannotate.audio](https://github.com/pyannote/pyannote-audio) and speechbrain with pip:
```
pip install pyannote.audio
pip install speechbrain
```
<br>
You also need to install [audiowaveform](https://github.com/bbc/audiowaveform#installation) and [ffmpeg](https://ffmpeg.org/download.html) (check `conda list` for ffmpeg, might've been installed automatically).

## Usage
Before running the server, you'll need to process some audio. To do so, run:
```
python process_audio.py FILE
```
For more information, run:
```
python process_audio.py -h
```
<br><br>
To start the server, run:
```
npm start
```
and then open http://localhost:3000 in your browser.

By default, the server listens on port 3000. To specify a different port, run with the port option:
```
npm start -- --port=PORT
```
where PORT is the port you want the server to listen on.
<br>
<br>
To actually display any audio, you need to:
1. Add the audio file to the `data/audio` directory.
2. Add the json file for the waveform (generated by the pipeline) to the `data/waveforms` directory.
3. Add the json file for the segments (generated by the pipeline) to the `data/segments` directory.

For example, for the file `example.mp3`, there should be `data/audio/example.mp3`, `data/waveforms/example-waveform.json`, and `data/segments/example-segments.json`.

## VRS Data Extraction
```
git clone https://github.com/facebookresearch/Aria_data_tools.git
$ docker build . --network=host -t aria_data_tools
$ docker run -it --volume <your_local_data>:/data aria_data_tools:latest
```

Reccomended you use bigcore for this as we have encountered VRS build issues on Windows 11/Mac os.

To get video from a VRS file, while in the container cd to the folder the vrs file is in. Then run:

```
vrs extract-images file.vrs --to image-folder + 1201-1
```

where + 1201-1 can be substituded for which sensor you want to get video from. 1201-1 is the first SLAM left camera complete list of sensors used with ARIA can be seen [here] (https://facebookresearch.github.io/Aria_data_tools/docs/sensors-measurements/#naming-conventions-for-all-tools)

Next cd to where ./create-video.sh is, in the main Speechviz folder then run

```
./create-video.sh path/to/image-folder
```

This will give you an mp4 of all the images extracted without any of the audio.

So the next step is audio extraction. For audio run:

```
vrs extract-audio file.vrs --to image-folder
```

Next write exit to leave the container. The next step is to use ffmpeg to rotate the video 90 degrees clockwise because by default the images are recorded rotated 90 degrees counterclockwise.

```
ffmpeg -i vid.mp4 -vf transpose=1 output.mp4
```

After that you will have to change the audio file you got from wav to mp3 so it can be combined with the mp4.

```
ffmpeg -i input-file.wav -vn -ar 44100 -ac 2 -b:a 192k output-file.mp3
```

Lastly combine the mp4 and mp3 to get a video with audio.

```
ffmpeg -i input.mp4 -i input.mp3 -c copy -map 0:v:0 -map 1:a:0 output.mp4
```

At this point if you want to then analyze the video with speechviz move the video to data/video and run

```
process_audio.py data/video
```

## Troubleshooting

If installing on Bigcore, you are likely to run into the following error:
```
ERROR: Could not install packages due to an OSError: Proxy URL had no scheme, should start with http:// or https://
```
To resolve this, run the following command:
```
http_proxy="http://$(echo $http_proxy)" && https_proxy="http://$(echo $https_proxy)"
```
<br>
<br>

If you receive this error:
```
subprocess.CalledProcessError: Command '['ffmpeg', '-y', '-i', 'input_file_here' 'output_file_here']' returned non-zero exit status 127.
```
Running
```
conda update ffmpeg
```
should resolve the issue.
