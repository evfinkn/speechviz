{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #EyeGazes: 7195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m[MultiRecordFileReader][DEBUG]: Opened file 'eedc2456-ccee-477a-916e-70a3a15cd004_blur/eedc2456-ccee-477a-916e-70a3a15cd004_blur.vrs' and assigned to reader #0\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 211-1/camera-et activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 214-1/camera-rgb activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 231-1/mic activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 247-1/baro0 activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 281-1/gps activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 282-1/wps activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 283-1/bluetooth activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;255;165;000m[VrsDataProvider][WARNING]: Unsupported TimeSync mode: APP, ignoring.\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-1/camera-slam-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1201-2/camera-slam-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-1/imu-right activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1202-2/imu-left activated\u001b[0m\n",
      "\u001b[0m\u001b[38;2;000;000;255m[VrsDataProvider][INFO]: streamId 1203-1/mag0 activated\u001b[0m\n",
      "\u001b[0mFailed to parse eye gaze vergence file: Extra column \"yaw_rads_cpf\" in header of file \"eedc2456-ccee-477a-916e-70a3a15cd004_blur/eyegaze/general_eye_gaze.csv\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from projectaria_tools.core import data_provider, mps\n",
    "from projectaria_tools.core.mps.utils import (\n",
    "    filter_points_from_confidence,\n",
    "    get_gaze_vector_reprojection,\n",
    "    get_nearest_eye_gaze,\n",
    "    get_nearest_pose,\n",
    ")\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "import numpy as np\n",
    "\n",
    "folder = \"eedc2456-ccee-477a-916e-70a3a15cd004_blur\"\n",
    "\n",
    "# Load the VRS file\n",
    "vrsfile = os.path.join(folder, \"eedc2456-ccee-477a-916e-70a3a15cd004_blur.vrs\")\n",
    "\n",
    "# Trajectory and global points\n",
    "# closed_loop_trajectory = os.path.join(\n",
    "#     folder, \"slam\", \"closed_loop_trajectory.csv\"\n",
    "# )\n",
    "# global_points = os.path.join(\"video1\", \"slam\", \"semidense_points.csv.gz\")\n",
    "\n",
    "# Eye gaze\n",
    "generalized_eye_gaze_path = os.path.join(\n",
    "    folder, \"eyegaze\", \"general_eye_gaze.csv\"\n",
    ")\n",
    "generalized_eye_gaze_depth_path = os.path.join(\n",
    "    folder, \"eyegaze\", \"gaze_with_depth.csv\"\n",
    ")\n",
    "\n",
    "# Hand tracking\n",
    "# wrist_and_palm_poses_path = os.path.join(\n",
    "#     folder, \"hand_tracking\", \"wrist_and_palm_poses.csv\"\n",
    "# )\n",
    "\n",
    "# Create data provider and get T_device_rgb\n",
    "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
    "# Since we want to display the position of the RGB camera, we are querying its relative location\n",
    "# from the device and will apply it to the device trajectory.\n",
    "T_device_RGB = provider.get_device_calibration().get_transform_device_sensor(\n",
    "    \"camera-rgb\"\n",
    ")\n",
    "\n",
    "## Load trajectory and global points\n",
    "# mps_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory)\n",
    "# points = mps.read_global_point_cloud(global_points)\n",
    "\n",
    "## Load eyegaze\n",
    "generalized_eye_gazes = mps.read_eyegaze(generalized_eye_gaze_path)\n",
    "\n",
    "## Load hand tracking\n",
    "# wrist_and_palm_poses = mps.hand_tracking.read_wrist_and_palm_poses(\n",
    "#     wrist_and_palm_poses_path\n",
    "# )\n",
    "\n",
    "# Loaded data must be not empty\n",
    "assert(\n",
    "    # len(mps_trajectory) != 0 and\n",
    "    # len(points) != 0 and\n",
    "    len(generalized_eye_gazes) != 0) #and\n",
    "    # len(wrist_and_palm_poses) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper function to build the frustum\n",
    "def build_cam_frustum(transform_world_device):\n",
    "    points = (\n",
    "        np.array(\n",
    "            [[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]\n",
    "        )\n",
    "        * 0.6\n",
    "    )\n",
    "    transform_world_rgb = transform_world_device @ T_device_RGB\n",
    "    points_transformed = transform_world_rgb @ points.transpose()\n",
    "    return go.Mesh3d(\n",
    "        x=points_transformed[0, :],\n",
    "        y=points_transformed[1, :],\n",
    "        z=points_transformed[2, :],\n",
    "        i=[0, 0, 0, 0, 1, 1],\n",
    "        j=[1, 2, 3, 4, 2, 3],\n",
    "        k=[2, 3, 4, 1, 3, 4],\n",
    "        showscale=False,\n",
    "        visible=False,\n",
    "        colorscale=\"jet\",\n",
    "        intensity=points[:, 2],\n",
    "        opacity=1.0,\n",
    "        hoverinfo=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load all world positions from the trajectory\n",
    "# traj = np.empty([len(mps_trajectory), 3])\n",
    "# for i in range(len(mps_trajectory)):\n",
    "#     traj[i, :] = mps_trajectory[i].transform_world_device.translation()\n",
    "\n",
    "# # Subsample trajectory for quick display\n",
    "# skip = 1000\n",
    "# mps_trajectory_subset = mps_trajectory[::skip]\n",
    "# steps = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "# # Load each pose as a camera frustum trace\n",
    "# cam_frustums = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "# for i in range(len(mps_trajectory_subset)):\n",
    "#     pose = mps_trajectory_subset[i]\n",
    "#     cam_frustums[i] = build_cam_frustum(pose.transform_world_device)\n",
    "#     timestamp = pose.tracking_timestamp.total_seconds()\n",
    "#     step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
    "#     step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "#     steps[i] = step\n",
    "# cam_frustums[0].visible = True\n",
    "    \n",
    "# # Filter the point cloud by inv depth and depth and load\n",
    "# points = filter_points_from_confidence(points)\n",
    "# # Retrieve point position\n",
    "# point_cloud = np.stack([it.position_world for it in points])\n",
    "\n",
    "# # Create slider to allow scrubbing and set the layout\n",
    "# sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
    "# layout = go.Layout(sliders=sliders, scene=dict(bgcolor='lightgray', dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
    "\n",
    "# # Plot trajectory and point cloud\n",
    "# # We color the points by their z coordinate\n",
    "# trajectory = go.Scatter3d(x=traj[:, 0], y=traj[:, 1], z=traj[:, 2], mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"}, name=\"Trajectory\", hoverinfo='none')\n",
    "# global_points = go.Scatter3d(x=point_cloud[:, 0], y=point_cloud[:, 1], z=point_cloud[:, 2], mode=\"markers\",\n",
    "#     marker={\"size\" : 1.5, \"color\": point_cloud[:, 2], \"cmin\": -1.5, \"cmax\": 2, \"colorscale\": \"viridis\",},\n",
    "#     name=\"Global Points\", hoverinfo='none')\n",
    "\n",
    "# # draw\n",
    "# plot_figure = go.Figure(data=cam_frustums + [trajectory, global_points], layout=layout)\n",
    "# plot_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "rgb_stream_label = provider.get_label_from_stream_id(rgb_stream_id)\n",
    "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
    "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames-5))\n",
    "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
    "\n",
    "image = rgb_frame[0].to_numpy_array()\n",
    "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
    "generalized_eye_gaze = get_nearest_eye_gaze(generalized_eye_gazes, capture_timestamp_ns)\n",
    "# get projection function\n",
    "device_calibration = provider.get_device_calibration()\n",
    "cam_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "assert cam_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "# fig, (ax1) = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "# # Draw a cross at the projected gaze center location on the RGB image at available depth or if unavailable a 1m proxy\n",
    "# depth_m = generalized_eye_gaze.depth or 1.0\n",
    "# generalized_gaze_center_in_pixels = get_gaze_vector_reprojection(generalized_eye_gaze, rgb_stream_label, device_calibration, cam_calibration, depth_m)\n",
    "# if generalized_gaze_center_in_pixels is not None:\n",
    "#     ax1.imshow(image)\n",
    "#     ax1.plot(generalized_gaze_center_in_pixels[0], generalized_gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "#     ax1.grid(False)\n",
    "#     ax1.axis(False)\n",
    "#     ax1.set_title(\"Generalized Eye Gaze\")\n",
    "# else:\n",
    "#     print(f\"Eye gaze center projected to {generalized_gaze_center_in_pixels}, which is out of camera sensor plane.\")\n",
    "\n",
    "\n",
    "# #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, concat, from 'subject/concat.txt':\n",
      "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 640x480 [SAR 1:1 DAR 4:3], 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x55988ddce680] deprecated pixel format used, make sure you did set range correctly\n",
      "[libx264 @ 0x55988dd7dd80] using SAR=1/1\n",
      "[libx264 @ 0x55988dd7dd80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55988dd7dd80] profile High, level 2.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55988dd7dd80] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=15 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'subject.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x480 [SAR 1:1 DAR 4:3], q=-1--1, 10 fps, 10240 tbn, 10 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame= 7193 fps=203 q=-1.0 Lsize=  170147kB time=00:11:59.00 bitrate=1938.6kbits/s speed=20.3x    \n",
      "video:170062kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.050363%\n",
      "[libx264 @ 0x55988dd7dd80] frame I:76    Avg QP:20.52  size: 46355\n",
      "[libx264 @ 0x55988dd7dd80] frame P:2066  Avg QP:22.57  size: 35205\n",
      "[libx264 @ 0x55988dd7dd80] frame B:5051  Avg QP:24.61  size: 19379\n",
      "[libx264 @ 0x55988dd7dd80] consecutive B-frames:  5.0%  3.2%  3.2% 88.7%\n",
      "[libx264 @ 0x55988dd7dd80] mb I  I16..4:  0.3% 94.5%  5.2%\n",
      "[libx264 @ 0x55988dd7dd80] mb P  I16..4:  0.2% 49.0%  1.7%  P16..4: 18.9% 17.2% 11.3%  0.0%  0.0%    skip: 1.7%\n",
      "[libx264 @ 0x55988dd7dd80] mb B  I16..4:  0.0% 16.6%  0.2%  B16..8: 40.5% 13.7%  5.5%  direct: 8.5%  skip:15.0%  L0:40.9% L1:38.5% BI:20.6%\n",
      "[libx264 @ 0x55988dd7dd80] 8x8 transform intra:97.1% inter:83.9%\n",
      "[libx264 @ 0x55988dd7dd80] coded y,uvDC,uvAC intra: 96.8% 0.7% 0.6% inter: 55.2% 1.4% 1.1%\n",
      "[libx264 @ 0x55988dd7dd80] i16 v,h,dc,p: 31% 30% 11% 28%\n",
      "[libx264 @ 0x55988dd7dd80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 11%  8% 43%  5%  6%  7%  6%  8%  7%\n",
      "[libx264 @ 0x55988dd7dd80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 17% 14%  6%  9% 10%  8%  7%  8%\n",
      "[libx264 @ 0x55988dd7dd80] i8c dc,h,v,p: 99%  0%  1%  0%\n",
      "[libx264 @ 0x55988dd7dd80] Weighted P-Frames: Y:12.7% UV:2.4%\n",
      "[libx264 @ 0x55988dd7dd80] ref P L0: 39.8% 14.8% 31.6% 12.8%  1.1%\n",
      "[libx264 @ 0x55988dd7dd80] ref B L0: 77.9% 17.8%  4.3%\n",
      "[libx264 @ 0x55988dd7dd80] ref B L1: 92.6%  7.4%\n",
      "[libx264 @ 0x55988dd7dd80] kb/s:1936.80\n",
      "ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/tmp/build/80754af9/ffmpeg_1587154242452/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --cc=/tmp/build/80754af9/ffmpeg_1587154242452/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --enable-avresample --enable-gmp --enable-hardcoded-tables --enable-libfreetype --enable-libvpx --enable-pthreads --enable-libopus --enable-postproc --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --disable-nonfree --enable-gpl --enable-gnutls --disable-openssl --enable-libopenh264 --enable-libx264\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'subject.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:11:59.30, start: 0.000000, bitrate: 1937 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480 [SAR 1:1 DAR 4:3], 1936 kb/s, 10 fps, 10 tbr, 10240 tbn, 20 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x564ab90b2f00] using SAR=1/1\n",
      "[libx264 @ 0x564ab90b2f00] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x564ab90b2f00] profile High, level 2.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x564ab90b2f00] 264 - core 157 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=20 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'subject_rotated.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 480x640 [SAR 1:1 DAR 3:4], q=-1--1, 10 fps, 10240 tbn, 10 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame= 7193 fps=239 q=-1.0 Lsize=  150060kB time=00:11:59.00 bitrate=1709.7kbits/s speed=23.8x    \n",
      "video:149975kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.056666%\n",
      "[libx264 @ 0x564ab90b2f00] frame I:75    Avg QP:20.20  size: 43471\n",
      "[libx264 @ 0x564ab90b2f00] frame P:2133  Avg QP:22.28  size: 31637\n",
      "[libx264 @ 0x564ab90b2f00] frame B:4985  Avg QP:24.45  size: 16616\n",
      "[libx264 @ 0x564ab90b2f00] consecutive B-frames:  5.8%  4.1%  4.0% 86.1%\n",
      "[libx264 @ 0x564ab90b2f00] mb I  I16..4:  0.9% 90.0%  9.1%\n",
      "[libx264 @ 0x564ab90b2f00] mb P  I16..4:  0.4% 41.2%  2.2%  P16..4: 22.6% 19.3% 12.2%  0.0%  0.0%    skip: 2.1%\n",
      "[libx264 @ 0x564ab90b2f00] mb B  I16..4:  0.0% 16.1%  0.2%  B16..8: 46.0% 13.1%  4.7%  direct: 5.1%  skip:14.8%  L0:41.2% L1:40.0% BI:18.8%\n",
      "[libx264 @ 0x564ab90b2f00] 8x8 transform intra:95.9% inter:81.8%\n",
      "[libx264 @ 0x564ab90b2f00] coded y,uvDC,uvAC intra: 96.7% 0.8% 0.7% inter: 47.6% 1.4% 1.0%\n",
      "[libx264 @ 0x564ab90b2f00] i16 v,h,dc,p: 31% 17% 12% 39%\n",
      "[libx264 @ 0x564ab90b2f00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  8% 10% 49%  5%  5%  5%  6%  6%  7%\n",
      "[libx264 @ 0x564ab90b2f00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 16% 15%  7%  8%  9%  9%  8%  8%\n",
      "[libx264 @ 0x564ab90b2f00] i8c dc,h,v,p: 99%  1%  1%  0%\n",
      "[libx264 @ 0x564ab90b2f00] Weighted P-Frames: Y:12.0% UV:1.5%\n",
      "[libx264 @ 0x564ab90b2f00] ref P L0: 46.9% 16.4% 25.6% 10.2%  0.8%\n",
      "[libx264 @ 0x564ab90b2f00] ref B L0: 83.8% 12.8%  3.5%\n",
      "[libx264 @ 0x564ab90b2f00] ref B L1: 94.8%  5.2%\n",
      "[libx264 @ 0x564ab90b2f00] kb/s:1708.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffmpeg', '-r', '10', '-i', 'subject.mp4', '-vf', 'transpose=1', 'subject_rotated.mp4'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "output_dir = \"subject\"\n",
    "\n",
    "slam_stream_id = StreamId(\"1201-1\")\n",
    "slam_stream_label = provider.get_label_from_stream_id(slam_stream_id)\n",
    "num_slam_frames = provider.get_num_data(slam_stream_id)\n",
    "slam_frame = provider.get_image_data_by_index(slam_stream_id, (int)(1))\n",
    "assert slam_frame[0] is not None, \"no slam frame\"\n",
    "\n",
    "image = slam_frame[0].to_numpy_array()\n",
    "prev_capture_timestamp_ns = slam_frame[1].capture_timestamp_ns\n",
    "time = prev_capture_timestamp_ns / 1e9\n",
    "\n",
    "# get projection function\n",
    "device_calibration = provider.get_device_calibration()\n",
    "cam_calibration = device_calibration.get_camera_calib(slam_stream_label)\n",
    "assert cam_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "# Loop over all the frames\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, 'concat.txt'), 'w') as f:\n",
    "    for i in range(1,num_slam_frames):\n",
    "        slam_frame = provider.get_image_data_by_index(slam_stream_id, (int)(i))\n",
    "        image = slam_frame[0].to_numpy_array()\n",
    "        capture_timestamp_ns = slam_frame[1].capture_timestamp_ns\n",
    "        time_diff = (capture_timestamp_ns - prev_capture_timestamp_ns) / 1e9\n",
    "        time = capture_timestamp_ns / 1e9\n",
    "\n",
    "        # Save the image\n",
    "        generalized_eye_gaze = get_nearest_eye_gaze(generalized_eye_gazes, capture_timestamp_ns)\n",
    "        depth_m = generalized_eye_gaze.depth or 1.0\n",
    "        depth, combined_yaw, combined_pitch = (\n",
    "            mps.compute_depth_and_combined_gaze_direction(\n",
    "                generalized_eye_gaze.vergence.left_yaw, generalized_eye_gaze.vergence.right_yaw, generalized_eye_gaze.pitch\n",
    "            )\n",
    "        )\n",
    "        generalized_gaze_center_in_pixels = get_gaze_vector_reprojection(generalized_eye_gaze, slam_stream_label, device_calibration, cam_calibration, depth_m)\n",
    "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
    "            # Convert the grayscale image to a color image\n",
    "            image_color = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            # The image is already a color image\n",
    "            image_color = image        \n",
    "        if generalized_gaze_center_in_pixels is not None:\n",
    "            # Draw a red cross on the image at the gaze center\n",
    "            marker_position = (int(generalized_gaze_center_in_pixels[0]), int(generalized_gaze_center_in_pixels[1]))\n",
    "            cv2.drawMarker(image_color, marker_position, color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=20, thickness=3)\n",
    "\n",
    "            # Add the depth as text in the top left corner of the image\n",
    "            depth_text = f\"Depth: {depth:.2f} meters\"\n",
    "            text_position = (550, image_color.shape[0] - 10)\n",
    "\n",
    "            for i, char in enumerate(depth_text):\n",
    "                # Create an image for the character\n",
    "                char_image = np.zeros_like(image_color)\n",
    "                cv2.putText(char_image, char, (text_position[0], text_position[1] - i * 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                # Rotate the character image 90 degrees counterclockwise\n",
    "                M = cv2.getRotationMatrix2D((text_position[0], text_position[1] - i * 20), 90, 1)\n",
    "                char_image_rotated = cv2.warpAffine(char_image, M, (char_image.shape[1], char_image.shape[0]))\n",
    "\n",
    "                # Add the rotated character image to the original image\n",
    "                mask = char_image_rotated > 0\n",
    "                image_color[mask] = char_image_rotated[mask]\n",
    "\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_dir, f'1201-1-{i}-{time:.3f}.jpg'), image_color)\n",
    "        f.write(f'file 1201-1-{i}-{time:.3f}.jpg\\n')\n",
    "        # Convert the image from BGR to RGB\n",
    "\n",
    "        # Display the image\n",
    "        #plt.imshow(image_color)\n",
    "        #plt.show()\n",
    "        #break\n",
    "        \n",
    "        prev_capture_timestamp_ns = capture_timestamp_ns\n",
    "# if output.mp4 exists delete it\n",
    "if os.path.exists('subject.mp4'):\n",
    "    os.remove('subject.mp4')\n",
    "if os.path.exists('subject_rotated.mp4'):\n",
    "    os.remove('subject_rotated.mp4')\n",
    "# Use ffmpeg to create the video with 10 fps\n",
    "subprocess.run(['ffmpeg', '-r', '10', '-f', 'concat', '-safe', '0', '-i', os.path.join(output_dir, 'concat.txt'), '-c:v', 'libx264', '-pix_fmt', 'yuv420p', 'subject.mp4'])\n",
    "subprocess.run(['ffmpeg', '-r', '10', '-i', 'subject.mp4', '-vf', 'transpose=1', 'subject_rotated.mp4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
